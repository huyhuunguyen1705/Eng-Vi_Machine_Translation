{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from config import get_config, latest_weights_file_path\n",
    "from train import get_model, get_ds, run_validation\n",
    "from translate import translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Max length of source sentence: 32\n",
      "Max length of target sentence: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14536\\1575033556.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_filename)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = latest_weights_file_path(config)\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The mobile phone is an instrument of freedom and an instrument of oppression .\n",
      "    TARGET: Điện thoại là phương tiện của tự do và cũng là phương tiện của áp bức .\n",
      " PREDICTED: Điện thoại là một công cụ tự do và một công cụ áp bức .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: What happens to American presidents at 0400 on inauguration day ?\n",
      "    TARGET: Điều gì xảy ra với các tổng thống Mỹ vào 4 giờ ngày nhậm chức ?\n",
      " PREDICTED: Điều gì xảy ra với tổng thống Mỹ vào ngày hôm nay ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: These include massage , dancing and praying .\n",
      "    TARGET: Các phương pháp này bao gồm mát-xa , khiêu vũ và cầu nguyện .\n",
      " PREDICTED: Những chiếc máy bay này nhảy lên và cầu nguyện .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Then one by one , she would encircle these thorns with dark ink .\n",
      "    TARGET: Bà lần lượt nối kết những cái gai với mực tối màu .\n",
      " PREDICTED: Sau đó , cô ấy sẽ làm những cái này với mực tối màu tối .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: All easier said than done .\n",
      "    TARGET: Nói dễ hơn làm .\n",
      " PREDICTED: Tất cả đều nói dễ hơn .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But they are not applicable in all situations .\n",
      "    TARGET: Nhưng không phải áp dụng được cho mọi trường hợp .\n",
      " PREDICTED: Nhưng họ không phải áp dụng được áp dụng cho mọi trường hợp .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And in fact , that &apos;s true of every ancient city .\n",
      "    TARGET: Và , đó là thực tế ở mọi thành thị cổ đại .\n",
      " PREDICTED: Và thực sự , đó là mọi thành phố cổ đại .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I said , &quot; He passed away a long time ago .\n",
      "    TARGET: Tôi nói &quot; Ông tôi cũng mất lâu rồi .\n",
      " PREDICTED: Tôi nói , & quot ; Anh ấy mất một thời gian dài .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: This is something we &apos;ve faced several times over the last few centuries .\n",
      "    TARGET: Đây là điều chúng ta đã gặp nhiều lần trong vài thế kỉ qua .\n",
      " PREDICTED: Đây là điều chúng ta đã gặp nhiều lần trong vài thế kỉ qua .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: So , take , for example , the three-way intersection .\n",
      "    TARGET: Lấy điểm giao cắt tại ngã 3 làm ví dụ .\n",
      " PREDICTED: Ví dụ , lấy cái cách ba giao diện .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The other dog doesn &apos;t have the button .\n",
      "    TARGET: Con chó kia thì không được trang bị cái nút như vậy .\n",
      " PREDICTED: Con chó kia không có nút bấm nào cả .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: FG : Ask Miriam !\n",
      "    TARGET: FG : Hãy hỏi Miriam !\n",
      " PREDICTED: FG : Hãy hỏi Miriam !\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Again , Venice is extraordinary for that .\n",
      "    TARGET: Một lần nữa , Venice lại là một ngoại lệ .\n",
      " PREDICTED: Một lần nữa , Venice lại là một sự bất thường .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Can you imagine ? Seriously , can you imagine ?\n",
      "    TARGET: Bạn có thể tưởng tượng được không ? Thiệt đó ?\n",
      " PREDICTED: Bạn có tưởng tượng được không ? Thật là nghiêm túc , bạn có thể tưởng tượng được không ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It gives you a sense of the trend .\n",
      "    TARGET: Tôi cho các bạn một cảm nhận về xu hướng .\n",
      " PREDICTED: Nó cho bạn một cảm nhận về xu hướng .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A little present for you .\n",
      "    TARGET: Một món quà nhỏ cho các bạn .\n",
      " PREDICTED: Một hiện tượng nhỏ cho bạn .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: My view is that museums should take a leaf out of the book of religions .\n",
      "    TARGET: Quan điểm của tôi là viện bảo tàng nên sao chép từ tôn giáo\n",
      " PREDICTED: Quan điểm của tôi là bảo tàng nên sao chép từ tôn giáo .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And you start having the same kind of traffic jams that you have in the ion channel .\n",
      "    TARGET: Bắt đầu xảy ra tắc nghẽn giao thông trong kênh ion\n",
      " PREDICTED: Và bạn bắt đầu có một giao thông mà bạn có trong kênh cá sấu .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It &apos;s the myth that we live in .\n",
      "    TARGET: Nó chính là thần thoại mà chúng ta đang sống trong đó .\n",
      " PREDICTED: Đó là bí ẩn mà chúng ta đang sống .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Why does this matter ?\n",
      "    TARGET: Tại sao điều này lại quan trọng chứ ?\n",
      " PREDICTED: Tại sao điều này lại quan trọng ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NguyenHuuHuy20220028\\Eng-Vi_Machine_Translation\\translate.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(model_filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SOURCE: \n",
      " PREDICTED:  "
     ]
    }
   ],
   "source": [
    "sentence = \"\"\n",
    "t = translate(sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
